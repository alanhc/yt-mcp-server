import os
from dotenv import load_dotenv
load_dotenv()

import glob
import uuid
import shutil
import yt_dlp
import re
import json
import hashlib
from pathlib import Path
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
from typing import List, Optional
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from faster_whisper import WhisperModel

# Anthropic client
try:
    from anthropic import Anthropic
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    if ANTHROPIC_API_KEY:
        anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)
        print("âœ… Anthropic API initialized")
    else:
        anthropic_client = None
        print("âš ï¸  ANTHROPIC_API_KEY not found")
except Exception as e:
    anthropic_client = None
    print(f"âš ï¸ Anthropic initialization failed: {e}")

# Global Whisper model (will be preloaded at startup)
WHISPER_MODEL = None
WHISPER_MODEL_SIZE = "base"

# Cache directory
CACHE_DIR = Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

def get_cache_key(url: str) -> str:
    """Generate cache key from URL"""
    return hashlib.md5(url.encode()).hexdigest()

def get_cached_result(url: str) -> Optional[dict]:
    """Retrieve cached result if exists"""
    cache_key = get_cache_key(url)
    cache_file = CACHE_DIR / f"{cache_key}.json"
    
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                print(f"âœ… Cache HIT for URL: {url[:50]}...")
                return cached_data
        except Exception as e:
            print(f"âš ï¸ Cache read error: {e}")
            return None
    
    print(f"âŒ Cache MISS for URL: {url[:50]}...")
    return None

def save_to_cache(url: str, result: dict):
    """Save result to cache"""
    cache_key = get_cache_key(url)
    cache_file = CACHE_DIR / f"{cache_key}.json"
    
    try:
        cache_data = {
            "url": url,
            "cached_at": datetime.now().isoformat(),
            "result": result
        }
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Saved to cache: {cache_file.name}")
    except Exception as e:
        print(f"âš ï¸ Cache write error: {e}")

def get_latest_episode_url(podcast_url: str) -> str:
    """Get the latest episode URL from an Apple Podcasts show page"""
    try:
        print(f"ğŸ” Fetching latest episode from: {podcast_url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(podcast_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'  # Force UTF-8 encoding
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        episode_links = soup.find_all('a', href=re.compile(r'/podcast/[^/]+/id\d+\?i=\d+'))
        
        if not episode_links:
            raise ValueError("No episodes found on the podcast page")
        
        latest_episode_url = episode_links[0]['href']
        if not latest_episode_url.startswith('http'):
            latest_episode_url = 'https://podcasts.apple.com' + latest_episode_url
            
        print(f"âœ… Found latest episode: {latest_episode_url}")
        return latest_episode_url
        
    except Exception as e:
        print(f"âŒ Error fetching latest episode: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Failed to fetch latest episode: {str(e)}"
        )

def get_podcast_episodes(podcast_url: str, max_episodes: int = 5) -> List[dict]:
    """Get recent episodes from an Apple Podcasts show page"""
    try:
        print(f"ğŸ” Fetching episodes from: {podcast_url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(podcast_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'  # Force UTF-8 encoding
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        episode_links = soup.find_all('a', href=re.compile(r'/podcast/[^/]+/id\d+\?i=\d+'))
        
        if not episode_links:
            raise ValueError("No episodes found on the podcast page")
            
        episodes = []
        seen_urls = set()
        
        for link in episode_links:
            url = link['href']
            if not url.startswith('http'):
                url = 'https://podcasts.apple.com' + url
            
            if url in seen_urls:
                continue
            seen_urls.add(url)
            
            title = link.get_text(strip=True)
            episodes.append({
                'title': title,
                'url': url
            })
            
            if len(episodes) >= max_episodes:
                break
                
        print(f"âœ… Found {len(episodes)} episodes")
        return episodes
        
    except Exception as e:
        print(f"âŒ Error fetching podcast episodes: {e}")
        raise HTTPException(status_code=400, detail=f"Failed to fetch episodes: {str(e)}")

def get_channel_videos(channel_url: str, max_videos: int = 5) -> List[dict]:
    """Get recent videos from a YouTube channel"""
    try:
        print(f"ğŸ” Fetching videos from channel: {channel_url}")
        
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': True,
            'playlistend': max_videos,
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(channel_url, download=False)
            
            if 'entries' not in info:
                raise ValueError("No videos found in channel")
            
            videos = []
            for entry in info['entries'][:max_videos]:
                if entry:
                    videos.append({
                        'title': entry.get('title', 'Untitled'),
                        'url': f"https://www.youtube.com/watch?v={entry['id']}"
                    })
            
            print(f"âœ… Found {len(videos)} videos")
            return videos
            
    except Exception as e:
        print(f"âŒ Error fetching channel videos: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Failed to fetch channel videos: {str(e)}"
        )

def split_text(text: str, max_chunk_size: int = 25000) -> List[str]:
    """Split text into chunks respecting line breaks"""
    chunks = []
    current_chunk = []
    current_length = 0
    
    lines = text.split('\n')
    for line in lines:
        line_len = len(line) + 1  # +1 for newline
        
        if current_length + line_len > max_chunk_size and current_chunk:
            chunks.append('\n'.join(current_chunk))
            current_chunk = []
            current_length = 0
            
        current_chunk.append(line)
        current_length += line_len
        
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    return chunks

def summarize_with_claude(content: str, prompt: str = None) -> str:
    """
    Summarize content using Claude AI with Map-Reduce support for long content.
    """
    if not anthropic_client:
        raise HTTPException(
            status_code=503,
            detail="AI summarization not available - ANTHROPIC_API_KEY not configured"
        )
    
    default_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆé–€å°‡ Podcast / YouTube é€å­—ç¨¿è½‰æˆã€ŒæŠ•è³‡æ•´ç†ï¼‹é‡é»æ‘˜è¦ã€çš„åŠ©æ‰‹ã€‚  
è«‹å…¨ç¨‹ä½¿ç”¨ **ç¹é«”ä¸­æ–‡** å›è¦†ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ã€è¼¸å…¥æ ¼å¼ã€‘
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æˆ‘æœƒå‚³çµ¦ä½ ä¸€å€‹ JSON ç‰©ä»¶ï¼Œçµæ§‹å¤§è‡´å¦‚ä¸‹ï¼š

{
  "url": "...",
  "cached_at": "...",
  "result": {
    "video_id": "...",
    "title": "...",
    "page": 1,
    "total_pages": 1,
    "transcribed_part": {
      "0": "â€¦",
      "3": "â€¦",
      "6": "â€¦",
      "...": "â€¦"
    }
  }
}

å…¶ä¸­ï¼š
- `transcribed_part` çš„ key æ˜¯ã€Œç§’æ•¸ï¼ˆstringï¼‰ã€ï¼Œvalue æ˜¯è©²æ™‚é–“é»çš„ä¸­æ–‡é€å­—ç¨¿ç‰‡æ®µã€‚
- å¯èƒ½åªæœ‰ä¸€é ï¼ˆpage=1, total_pages=1ï¼‰ã€‚

ä½ éœ€è¦å…ˆæŠŠ `transcribed_part` ä¾ç…§ keyï¼ˆç§’æ•¸ï¼‰ç”±å°åˆ°å¤§æ’åºï¼Œä¸²æˆä¸€ä»½å®Œæ•´é€å­—ç¨¿ï¼Œå†é€²è¡Œå¾ŒçºŒåˆ†æã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ã€ä»»å‹™ã€‘
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è«‹é‡å°æ•´æ®µå…§å®¹ç”¢å‡ºä¸€ä»½ã€ŒæŠ•è³‡å‘ã€æ‘˜è¦èˆ‡æ¨™çš„æ•´ç†ï¼Œä¸¦ **ä¿ç•™é—œéµå¥çš„æ™‚é–“æ¨™è¨˜**ã€‚  
æ™‚é–“è«‹æŠŠç§’æ•¸è½‰æˆ `mm:ss` æ ¼å¼ï¼ˆä¾‹å¦‚ "17" â†’ "00:17"ï¼‰ã€‚

è¼¸å‡ºçµæ§‹è«‹å›ºå®šç‚ºä»¥ä¸‹ 5 å€‹å€å¡Šï¼š

1ï¸âƒ£ ç¯€ç›®ç¸½çµï¼ˆé«˜å±¤æ‘˜è¦ï¼‰
- ç”¨ 3â€“7 é»æ¢åˆ—èªªæ˜é€™é›†åœ¨è¬›ä»€éº¼ï¼Œä¸»è¦è„ˆçµ¡æ˜¯ä»€éº¼ã€‚
- å…ˆç°¡å–®æåˆ°ä¸€é–‹å§‹çš„ç½®å…¥ï¼ˆä¾‹å¦‚ Sony WH-1000XM6 è€³æ©Ÿï¼‰ï¼Œå†èªªæ˜å¾Œé¢ä¸»ç·šï¼š  
  - éŠæˆ²æŠ•è³‡ / å°ç”·è³‡æœ¬èˆ‡å±‹å±±è³‡æœ¬  
  - å°ç£éŠæˆ²åœ˜éšŠèˆ‡æ–°å‰µéŠæˆ²  
  - Google / TPU / Broadcom / MediaTek / TSMC ç­‰ AI ä¾›æ‡‰éˆåŠæŠ•è³‡è§€é»  
  - æŠ•è³‡å¿ƒæ…‹ã€æ§“æ¡¿ã€é¢¨éšªèˆ‡è³‡é‡‘æ§ç®¡  
  - QA æ®µè½ä¸­èˆ‡æŠ•è³‡ã€å·¥ä½œã€å®¶åº­ç›¸é—œçš„é‡é»æƒ³æ³•

2ï¸âƒ£ æŠ•è³‡ä¸»é¡Œèˆ‡é—œéµè«–é»
ç”¨å°æ¨™ï¼‹æ¢åˆ—æ•´ç†ï¼Œèˆ‰ä¾‹ï¼š

- ğŸ® éŠæˆ²æŠ•è³‡èˆ‡å°ç”·è³‡æœ¬
  - èªªæ˜å°ç”·è³‡æœ¬ã€å±‹å±±è³‡æœ¬åœ¨åšä»€éº¼ï¼ˆå‹Ÿè³‡ã€æŠ•è³‡å°è±¡ã€Evergreen Fund æ¦‚å¿µï¼‰ã€‚
  - å°ç£éŠæˆ²åœ˜éšŠç›®å‰é¢è‡¨çš„æˆæœ¬ã€ç™¼è¡Œã€å‰µæ„èˆ‡ç”Ÿå­˜å•é¡Œã€‚
  - ä¸»æŒäººåœ¨ NMEA äºæ´²æ–°åª’é«”é«˜å³°æœƒä¸Šä¸»å¼µçš„æ ¸å¿ƒç†å¿µï¼ˆéŠæˆ²æ€§ > ç•«é¢ã€ä¸è¦æ•™è‚²ç©å®¶ç­‰ï¼‰ã€‚

- ğŸ¤– Google TPU / Broadcom / MediaTek / NVIDIA / TSMC ä¾›æ‡‰éˆ
  - æ•´ç†ä¸»æŒäººå° Google TPUã€Broadcomã€MediaTekï¼ˆè¯ç™¼ç§‘ï¼‰ã€TSMC çš„è§’è‰²èˆ‡ç‡Ÿæ”¶çµæ§‹æè¿°ã€‚
  - èªªæ˜ã€ŒASIC vs GPUã€çš„å°æ¯”è§€é»ï¼šç‚ºä½•ä¸èƒ½ç°¡å–®è§£è®€æˆã€ŒASIC è´ã€GPU æ­»ã€ã€‚
  - èªªæ˜ TPU ä»£æ•¸ï¼ˆå¦‚ V7ï¼‰èˆ‡ç”¢èƒ½ / CoWoS / HBM / å°æ¸¬å» ç›¸é—œçš„ä¾›æ‡‰éˆå»¶ä¼¸ã€‚
  - æ•´ç†ä¸»æŒäººå° NVIDIA è‚¡åƒ¹ã€æ‹‰å›ã€é•·æœŸé…ç½®çš„çœ‹æ³•ã€‚

- ğŸ“ˆ æŠ•è³‡å¿ƒæ…‹èˆ‡æ“ä½œåŸå‰‡
  - å°ã€Œä¸è¦ç•¶å˜´ç ²å®…ï¼Œè¦çœŸçš„æ‹¿éŒ¢å‡ºä¾†æŠ•è³‡ã€çš„è§€é»ã€‚
  - å°ã€Œä¸è¦æŠŠæŠ•è³‡ç•¶çƒè³½ã€ä¸è¦é A æ­» B çš„äºŒå…ƒæ€ç¶­ã€çš„æ‰¹è©•ã€‚
  - æ€éº¼çœ‹å¾…æ§“æ¡¿ã€å›æª”ã€è¿½é«˜ã€æ¶ˆæ¯é¢äº¤æ˜“ã€å°ä½œæ–‡ã€Xï¼ˆTwitterï¼‰è³‡è¨Šã€‚
  - å°æ•£æˆ¶å®¹æ˜“å‡ºç¾çš„å¿ƒæ…‹éŒ¯èª¤èˆ‡å»ºè­°ï¼ˆåˆ‡å¸³æˆ¶ã€è³‡é‡‘åˆ†å±¤ã€ä¸è¦è‡ªè¦–ç”šé«˜ç­‰ï¼‰ã€‚

3ï¸âƒ£ æŠ•è³‡æ¨™çš„æ¸…å–®ï¼ˆæ¨™çš„ x ç‹€æ³ï¼‰
è«‹åšæˆ Markdown è¡¨æ ¼ï¼Œæ•´ç†ç¯€ç›®ä¸­ **èˆ‡æŠ•è³‡ç›¸é—œçš„æ¨™çš„**ï¼ŒåŒ…å«ä½†ä¸é™æ–¼ï¼š

- å€‹è‚¡ï¼å…¬å¸ï¼šGoogleã€NVIDIAã€Broadcomã€MediaTekï¼ˆè¯ç™¼ç§‘ï¼‰ã€TSMCã€AWSã€Microsoftã€Metaâ€¦  
- é¡åˆ¥ï¼ç”¢å“ï¼šTPUã€GPUã€HBMã€CoWoSã€å°æ¸¬å» ï¼ˆä¾‹å¦‚ Amkorã€SPIL ç­‰å¦‚æœæœ‰æï¼‰ã€Google ä¾›æ‡‰éˆã€AI ä¼ºæœå™¨ã€éŠæˆ²è‚¡/éŠæˆ²åœ˜éšŠ  
- å…¶ä»–ï¼šSony WH-1000XM6ï¼ˆé›–ç„¶æ˜¯å»£å‘Šç”¢å“ï¼Œä½†ä¹Ÿå¯ç°¡åˆ—æ–¼ã€Œç”¢å“ã€é¡ï¼‰

è¡¨æ ¼æ¬„ä½è«‹å¦‚ä¸‹ï¼š

| é¡å‹ | åç¨± / ä»£è™Ÿ | åœ¨ç¯€ç›®ä¸­çš„è§’è‰² / å®šä½ | ä¸»æŒäººè§€é»ï¼ˆåå¤š / åç©º / ä¸­æ€§ï¼‰ | é—œéµè«–é»æ‘˜è¦ | ç›¸é—œæ™‚é–“ï¼ˆmm:ss, å¯å¤šå€‹ï¼‰ |
| ---- | ---------- | ---------------------- | ---------------------------------- | ------------ | -------------------------- |

è¦å‰‡ï¼š
- åªåˆ—å‡ºé€å­—ç¨¿ä¸­çœŸçš„æœ‰æåˆ°ã€ä¸”èˆ‡ã€ŒæŠ•è³‡ / ç”¢æ¥­ã€æœ‰é—œçš„æ¨™çš„ã€‚  
- è‹¥ç¯€ç›®å°æŸæ¨™çš„æ²’æœ‰æ˜é¡¯å¤šç©ºå‚¾å‘ï¼Œå°±å¯«ã€Œä¸­æ€§ã€æˆ–ã€Œæè¿°ç‚ºä¸»ã€å³å¯ã€‚  
- `ç›¸é—œæ™‚é–“` æ¬„ï¼šå¡«å…¥ 1~3 å€‹æœ€é—œéµçš„æ™‚é–“é»ï¼ˆå¾ transcribed_part å°æ‡‰çš„ key æ›ç®—æˆ mm:ssï¼‰ã€‚

4ï¸âƒ£ æŠ•è³‡å¿ƒæ…‹èˆ‡æ“ä½œå»ºè­°ï¼ˆå¾ç¯€ç›®èƒå–ï¼‰
- ä¸æ˜¯è¦ä½ çµ¦æˆ‘è‡ªå·±çš„æŠ•é¡§æ„è¦‹ï¼Œè€Œæ˜¯ **æ•´ç†ä¸»æŒäººè‡ªå·±è¬›çš„æ“ä½œå¿ƒæ³•**ã€‚
- ç”¨æ¢åˆ—æ–¹å¼ï¼Œåˆ†æˆï¼š
  - ã€Œè³‡é‡‘èˆ‡æ§“æ¡¿ã€ï¼šä¾‹å¦‚ä¸‰å€æ§“æ¡¿ã€å›æª”æ‰¿å—åº¦ã€ä¸è¦ä»¥ç‚ºè‡ªå·±èƒ½è²·åœ¨ä¸»åŠ›å‰é¢ç­‰ã€‚
  - ã€Œç”¢æ¥­èˆ‡é¡Œæã€ï¼šä¾‹å¦‚ Google ä¹‹äº‚ã€TPU vs GPUã€ä¸è¦éåº¦è§£è®€çŸ­æœŸåˆ©å¤š/åˆ©ç©ºã€‚
  - ã€Œæ•£æˆ¶å¸¸è¦‹é™·é˜±ã€ï¼šè¿½é«˜ã€äº‚ä¸‹å–®ã€æƒ…ç·’åŒ–äº¤æ˜“ã€å°ä½œæ–‡ã€å°é™æ¯ï¼å®è§€äº‹ä»¶çš„éŒ¯èª¤æƒ³åƒã€‚
- æ¯é»ç›¡é‡é™„ä¸Š **å°æ‡‰çš„å¤§è‡´æ™‚é–“**ï¼ˆmm:ssï¼‰æ–¹ä¾¿å›è½ã€‚

5ï¸âƒ£ ç²¾é¸é‡‘å¥ï¼ˆå«æ™‚é–“ï¼‰
- ç¯©é¸ 5â€“10 å¥æœ€å€¼å¾—è¨˜ä¸‹ä¾†çš„å¥å­ï¼Œå¯ä»¥æ˜¯ï¼š
  - å°æŠ•è³‡å¿ƒæ…‹å¾ˆæœ‰å¹«åŠ©çš„è©±
  - å°ç”¢æ¥­ / TPU / GPU / Google ä¾›æ‡‰éˆæœ‰æ´è¦‹çš„è©•è«–
  - å°å®¶åº­ã€å­©å­ã€äººç”Ÿã€å·¥ä½œç­‰å¾ˆæœ‰æ„Ÿè§¸çš„è©±
- æ¯ä¸€æ¢æ ¼å¼å¦‚ä¸‹ï¼š
  - `mm:ss â€“ ã€ŒåŸæ–‡é‡‘å¥ã€`  
- ä¸éœ€è¦é€å­—å®Œå…¨ä¸€è‡´ï¼Œä½†è¦æ¥è¿‘æ—¥å¸¸å£èªï¼Œä¸è¦äº‚æ”¹æ„æ€ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ã€é¢¨æ ¼èˆ‡é™åˆ¶ã€‘
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- å…¨ç¨‹ä½¿ç”¨ **ç¹é«”ä¸­æ–‡**ã€‚
- ä¸è¦è‡ªå·±å‰µé€ é€å­—ç¨¿è£¡ä¸å­˜åœ¨çš„æ¨™çš„æˆ–æ•¸å­—ï¼›ç„¡æ³•ç¢ºå®šå°±å¯«ã€Œæœªæ˜ç¢ºèªªæ˜ã€ã€‚
- å¯ä»¥é©åº¦ç°¡åŒ–å£èªï¼Œä½†ä¿ç•™åŸæœ¬èªæ°£èˆ‡ç«‹å ´ã€‚
- ä½ æ˜¯ã€Œç¯€ç›®æ•´ç†è€…ã€ï¼Œä¸æ˜¯æŠ•é¡§ï¼Œ**è«‹ä¸è¦å°æ¨™çš„ä¸‹è²·é€²/è³£å‡ºæŒ‡ä»¤**ï¼Œåªéœ€å¿ å¯¦æ•´ç†ç¯€ç›®å…§å®¹èˆ‡è§€é»ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ã€å¯¦éš›è¼¸å…¥ã€‘
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä»¥ä¸‹æ˜¯æœ¬æ¬¡è¦è™•ç†çš„ JSON è¼¸å…¥ï¼š



"""
    base_prompt = prompt or default_prompt

    def call_claude(text_chunk):
        full_prompt = base_prompt + "\n\n" + text_chunk
        try:
            message = anthropic_client.messages.create(
                model="claude-3-5-haiku-latest",
                max_tokens=8192,  # Max allowed for Haiku
                messages=[
                    {"role": "user", "content": full_prompt}
                ]
            )
            return message.content[0].text
        except Exception as e:
            print(f"âš ï¸ Claude API error for chunk: {e}")
            return f"[Error summarizing this chunk: {str(e)}]"

    # Check if content needs splitting (approx 30k chars)
    if len(content) > 30000:
        print(f"ğŸ“¦ Content length {len(content)} exceeds limit, using Map-Reduce summarization...")
        chunks = split_text(content)
        chunk_summaries = []
        chunks_metadata = []
        
        # Map Phase: Summarize each chunk (with caching)
        for i, chunk in enumerate(chunks):
            # Generate cache key for this chunk
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            chunk_cache_file = CACHE_DIR / f"chunk_{chunk_hash}.json"
            
            chunk_data = None
            
            # Check chunk cache
            if chunk_cache_file.exists():
                try:
                    with open(chunk_cache_file, 'r', encoding='utf-8') as f:
                        cached_chunk = json.load(f)
                        chunk_summary = cached_chunk['summary']
                        chunk_data = cached_chunk
                        print(f"âœ… [Map] Chunk {i+1}/{len(chunks)} cache HIT")
                except Exception as e:
                    print(f"âš ï¸ Chunk cache read error: {e}")
            
            if not chunk_data:
                print(f"ğŸ”„ [Map] Processing chunk {i+1}/{len(chunks)} ({len(chunk)} chars)...")
                chunk_summary = call_claude(chunk)
                
                # Save chunk summary to cache
                try:
                    chunk_data = {
                        'chunk_hash': chunk_hash,
                        'cached_at': datetime.now().isoformat(),
                        'summary': chunk_summary,
                        'chunk_length': len(chunk)
                    }
                    with open(chunk_cache_file, 'w', encoding='utf-8') as f:
                        json.dump(chunk_data, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ Saved chunk {i+1} to cache")
                except Exception as e:
                    print(f"âš ï¸ Failed to cache chunk: {e}")
                    # Create temporary chunk data even if cache failed
                    chunk_data = {
                        'chunk_hash': chunk_hash,
                        'cached_at': datetime.now().isoformat(),
                        'summary': chunk_summary,
                        'chunk_length': len(chunk)
                    }
            
            chunk_summaries.append(chunk_summary)
            chunks_metadata.append(chunk_data)
            
        # Reduce Phase: Concatenate summaries + Generate final overview
        if len(chunk_summaries) > 1:
            print("ğŸ”„ [Reduce] Concatenating chunk summaries...")
            # Step 1: Direct concatenation
            concatenated_summaries = "\n\n---\n\n".join([
                f"### ç¬¬ {i+1} éƒ¨åˆ†\n{summary}" 
                for i, summary in enumerate(chunk_summaries)
            ])
            
            # Step 2: Generate final overview based on concatenated content
            print("ğŸ”„ [Reduce] Generating final overview summary...")
            overview_prompt = f"""è«‹åŸºæ–¼å…¨éƒ¨ chunkï¼Œç”¢ç”Ÿä¸€ä»½ **æŠ•è³‡å‘ã€å¯å›è½ã€å¯åšç­†è¨˜** çš„é«˜å“è³ªç¸½çµï¼ŒåŒ…å«ï¼š

1ï¸âƒ£ ç¯€ç›®ç¸½çµï¼ˆ3â€“7 å¥ï¼ŒæŠ“å‡ºä¸»è»¸ï¼‰  
2ï¸âƒ£ æ·±åº¦æŠ•è³‡é‡é»æ•´ç†ï¼ˆæ¢åˆ—ï¼‹å°æ¨™é¡Œï¼‰  
   - éŠæˆ²æŠ•è³‡ã€å°ç£åœ˜éšŠã€åŸºé‡‘ç†å¿µ  
   - AI/TPU/GPU/ASICã€ä¾›æ‡‰éˆï¼ˆBroadcom / MTK / NVIDIA / TSMC ç­‰ï¼‰  
   - æŠ•è³‡å¿ƒæ…‹ã€æ•£æˆ¶éŒ¯èª¤ã€æ§“æ¡¿èˆ‡é¢¨éšª  
3ï¸âƒ£ æŠ•è³‡æ¨™çš„è¡¨ï¼ˆå¾æ‰€æœ‰ chunks çš„ entities å½™ç¸½ï¼‰  
   - é¡å‹ï¼ˆå…¬å¸ / ç”¢å“ / æ¦‚å¿µï¼‰  
   - åç¨±  
   - ç¯€ç›®ä¸­çš„å®šä½  
   - ä¸»æŒäººæ…‹åº¦ï¼ˆåå¤š / åç©º / ä¸­æ€§ï¼‰  
   - å°æ‡‰ chunk çš„ timestampsï¼ˆmm:ss æ ¼å¼å³å¯ï¼‰  
4ï¸âƒ£ æŠ•è³‡å¿ƒæ…‹èˆ‡æ“ä½œå»ºè­°ï¼ˆä¸»æŒäººè§€é»ï¼Œä¸æ˜¯ä½ çš„å»ºè­°ï¼‰  
5ï¸âƒ£ ç²¾é¸é‡‘å¥ï¼ˆå¾ raw_sentences ä¸­æŒ‘æœ€æœ‰åŠ›çš„ 5â€“10 å¥ï¼‰

ã€è¦å‰‡ã€‘
- è‹¥å¤šå€‹ chunk é‡è¤‡å…§å®¹ï¼Œè¦ã€Œæ•´ä½µã€è€Œä¸æ˜¯é‡è¤‡è²¼ä¸Šã€‚
- è‹¥è§€é»æœ‰çŸ›ç›¾ï¼Œè«‹åšã€Œæ•´åˆæ€§è§£é‡‹ã€ã€‚
- å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

ä»¥ä¸‹æ˜¯åˆ†æ®µæ‘˜è¦å…§å®¹ï¼š

{concatenated_summaries}
"""
            
            final_output = call_claude(overview_prompt)
            return final_output, chunks_metadata
        else:
            return chunk_summaries[0], chunks_metadata
    else:
        # Single chunk processing
        summary = call_claude(content)
        return summary, []
    
@asynccontextmanager
async def lifespan(app: FastAPI):
    global WHISPER_MODEL
    print("ğŸš€ Starting up server...")
    print(f"ğŸ“¥ Preloading faster_whisper model: {WHISPER_MODEL_SIZE}")
    print("âš¡ GPU-ONLY MODE: CPU fallback disabled")
    try:
        print("Loading model on CUDA...")
        WHISPER_MODEL = WhisperModel(
            WHISPER_MODEL_SIZE,
            device="cuda",
            compute_type="float16"
        )
        print("âœ… Model preloaded successfully on GPU!")
    except Exception as e:
        print(f"âŒ FATAL: Could not load model on GPU: {e}")
        print("ğŸ’¡ Ensure CUDA and cuDNN are properly installed")
        raise RuntimeError(f"GPU initialization failed: {e}")
    
    yield
    print("ğŸ‘‹ Shutting down server...")

app = FastAPI(
    title="yt-mcp-server",
    lifespan=lifespan,
    servers=[
        {
            "url": "https://be.0xfanslab.com",
            "description": "Public HTTPS endpoint"
        }
    ],
)

class VideoRequest(BaseModel):
    url: str
    lang: str | None = None

def parse_vtt(vtt_content):
    lines = vtt_content.split('\n')
    cues = []
    
    time_pattern = re.compile(r'(\d{2}:)?(\d{2}):(\d{2})\.(\d{3})')
    
    current_start = None
    buffer = []
    
    for line in lines:
        line = line.strip()
        
        if '-->' in line:
            match = time_pattern.search(line)
            if match:
                groups = match.groups()
                hours = int(groups[0][:-1]) if groups[0] else 0
                minutes = int(groups[1])
                seconds = int(groups[2])
                timestamp = hours * 3600 + minutes * 60 + seconds
                
                if buffer and current_start is not None:
                    cues.append({'start': current_start, 'text': ' '.join(buffer)})
                    buffer = []
                
                current_start = timestamp
        
        elif line and not line.startswith('WEBVTT') and not line.isdigit() and current_start is not None:
            buffer.append(line)
    
    if buffer and current_start is not None:
        cues.append({'start': current_start, 'text': ' '.join(buffer)})
    
    return cues

@app.post("/yt")
def get_subtitles(request: VideoRequest):
    print(f"Processing request for URL: {request.url} with lang: {request.lang}")
    url = request.url
    lang = request.lang or "zh-TW"
    
    cached_result = get_cached_result(url)
    if cached_result:
        return cached_result["result"]
    
    # First, try to get subtitles
    ydl_opts = {
        'writesubtitles': True,
        'writeautomaticsub': True,
        'subtitleslangs': [lang],
        'skip_download': True,
        'outtmpl': '/tmp/%(id)s.%(ext)s',
        'quiet': True,
        'no_warnings': True,
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            video_id = info.get('id')
            title = info.get('title')
            
            pattern = f"/tmp/{video_id}.{lang}.vtt"
            vtt_files = glob.glob(pattern)
            
            if not vtt_files:
                pattern = f"/tmp/{video_id}.*.vtt"
                vtt_files = glob.glob(pattern)
            
            if vtt_files:
                # Subtitles found, use them
                print(f"âœ… Found subtitles for {video_id}")
                vtt_file = vtt_files[0]
                
                with open(vtt_file, 'r', encoding='utf-8') as f:
                    vtt_content = f.read()
                
                cues = parse_vtt(vtt_content)
                
                transcribed_part = {}
                for cue in cues:
                    transcribed_part[cue['start']] = cue['text']
                
                result = {
                    "video_id": video_id,
                    "title": title,
                    "page": 1,
                    "total_pages": 1,
                    "transcribed_part": transcribed_part,
                    "transcription_method": "subtitles"
                }
                
                save_to_cache(url, result)
                
                for f in glob.glob(f"/tmp/{video_id}.*"):
                    try:
                        os.remove(f)
                    except:
                        pass
                
                return result
            else:
                # No subtitles found, fallback to audio transcription
                print(f"âš ï¸ No subtitles found for {video_id}, falling back to audio transcription...")
                
    except Exception as e:
        print(f"âš ï¸ Error getting subtitles: {e}, falling back to audio transcription...")
    
    # Fallback: Download audio and transcribe with Whisper
    print(f"ğŸµ Downloading audio for transcription: {url}")
    request_id = str(uuid.uuid4())
    temp_dir = f"/tmp/{request_id}"
    os.makedirs(temp_dir, exist_ok=True)
    
    ydl_opts_audio = {
        'format': 'bestaudio/best',
        'outtmpl': f'{temp_dir}/%(id)s.%(ext)s',
        'quiet': True,
        'no_warnings': True,
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts_audio) as ydl:
            info = ydl.extract_info(url, download=True)
            video_id = info.get('id')
            title = info.get('title', 'Untitled Video')
            
            files = glob.glob(f"{temp_dir}/{video_id}.*")
            if not files:
                raise HTTPException(status_code=404, detail="Failed to download audio file.")
            
            audio_file = files[0]
            file_size = os.path.getsize(audio_file)
            print(f"ğŸ“¥ Downloaded audio file: {audio_file}, size: {file_size} bytes")
            
            if file_size == 0:
                raise HTTPException(status_code=500, detail="Downloaded audio file is empty.")
            
            try:
                import time
                file_size_mb = file_size / (1024 * 1024)
                estimated_time = int(file_size_mb * 2)
                print(f"ğŸ™ï¸ Starting transcription with faster_whisper (GPU)...")
                print(f"File size: {file_size_mb:.1f}MB, estimated time: ~{estimated_time}s")
                
                global WHISPER_MODEL
                if WHISPER_MODEL is None:
                    raise RuntimeError("GPU model not loaded. Server startup may have failed.")
                
                start_time = time.time()
                segments, info = WHISPER_MODEL.transcribe(audio_file, beam_size=5)
                elapsed_time = time.time() - start_time
                print(f"âœ… Transcription completed in {elapsed_time:.1f}s")
            except Exception as e:
                import traceback
                traceback.print_exc()
                raise HTTPException(status_code=500, detail=f"Whisper transcription failed: {str(e)}")
            
            transcribed_part = {}
            for segment in segments:
                start = int(segment.start)
                text = segment.text.strip()
                transcribed_part[start] = text
            
            result = {
                "video_id": video_id,
                "title": title,
                "page": 1,
                "total_pages": 1,
                "transcribed_part": transcribed_part,
                "transcription_method": "whisper"
            }
            
            save_to_cache(url, result)
            
            return result
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

class PodcastRequest(BaseModel):
    url: str
    lang: str | None = None

@app.post("/apple_podcast/latest")
def get_latest_podcast_episode(request: PodcastRequest):
    """Accepts a podcast show URL and returns the subtitles for the latest episode"""
    podcast_url = request.url
    print(f"Processing latest episode request for podcast: {podcast_url}")

    try:
        latest_episode_url = get_latest_episode_url(podcast_url)
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error fetching latest episode info: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Failed to find latest episode: {str(e)}"
        )
    
    episode_request = VideoRequest(url=latest_episode_url, lang=request.lang)
    result = get_apple_podcast_subtitles(episode_request)
    
    result["podcast_show_url"] = podcast_url
    result["episode_url"] = latest_episode_url
    result["is_latest_episode"] = True
    
    # Ensure view_url is present
    if "view_url" not in result:
        view_base_url = "https://be.0xfanslab.com/youtube/channel/summary"
        cache_key = get_cache_key(latest_episode_url)
        result["view_url"] = f"{view_base_url}?id={cache_key}&type=podcast"
    
    return result

@app.get("/api/podcast/summary/{cache_key}")
def get_podcast_cache(cache_key: str):
    """Retrieve cached podcast summary by cache key"""
    cache_file = CACHE_DIR / f"{cache_key}.json"
    
    if not cache_file.exists():
        raise HTTPException(status_code=404, detail="Cache not found")
    
    try:
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
            return cache_data.get("result", {})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read cache: {str(e)}")

@app.post("/apple_podcast")
def get_apple_podcast_subtitles(request: VideoRequest):
    """Accepts an Apple Podcast URL and returns the subtitles using faster_whisper"""
    print(f"Processing Apple Podcast request for URL: {request.url}")
    url = request.url
    
    view_base_url = "https://be.0xfanslab.com/youtube/channel/summary"
    
    cached_result = get_cached_result(url)
    if cached_result:
        result = cached_result["result"]
        cache_key = get_cache_key(url)
        result["view_url"] = f"{view_base_url}?id={cache_key}&type=podcast"
        return result
    
    request_id = str(uuid.uuid4())
    temp_dir = f"/tmp/{request_id}"
    os.makedirs(temp_dir, exist_ok=True)
    
    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': f'{temp_dir}/%(id)s.%(ext)s',
        'quiet': True,
        'no_warnings': True,
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            video_id = info.get('id')
            title = info.get('title', 'Untitled Podcast')
            
            files = glob.glob(f"{temp_dir}/{video_id}.*")
            if not files:
                raise HTTPException(status_code=404, detail="Failed to download audio file.")
            
            audio_file = files[0]
            file_size = os.path.getsize(audio_file)
            print(f"Downloaded audio file: {audio_file}, size: {file_size} bytes")
            
            if file_size == 0:
                 raise HTTPException(status_code=500, detail="Downloaded audio file is empty.")

            try:
                import time
                file_size_mb = file_size / (1024 * 1024)
                estimated_time = int(file_size_mb * 2)
                print(f"Starting transcription with faster_whisper (GPU)...")
                print(f"File size: {file_size_mb:.1f}MB, estimated time: ~{estimated_time}s")
                
                global WHISPER_MODEL
                if WHISPER_MODEL is None:
                    raise RuntimeError("GPU model not loaded. Server startup may have failed.")
                
                start_time = time.time()
                segments, info = WHISPER_MODEL.transcribe(audio_file, beam_size=5)
                elapsed_time = time.time() - start_time
                print(f"âœ… Transcription completed in {elapsed_time:.1f}s")
            except Exception as e:
                import traceback
                traceback.print_exc()
                raise HTTPException(status_code=500, detail=f"Whisper transcription failed: {str(e)}")
            
            transcribed_part = {}
            for segment in segments:
                start = int(segment.start)
                text = segment.text.strip()
                transcribed_part[start] = text
                
            result = {
                "video_id": video_id,
                "title": title,
                "page": 1,
                "total_pages": 1,
                "transcribed_part": transcribed_part
            }
            
            save_to_cache(url, result)
            
            cache_key = get_cache_key(url)
            result["view_url"] = f"{view_base_url}?id={cache_key}&type=podcast"
            
            return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

class VideoSummaryRequest(BaseModel):
    url: str
    custom_prompt: Optional[str] = None

@app.post("/youtube/summary")
def summarize_youtube_video(request: VideoSummaryRequest):
    """Summarize a single YouTube video"""
    print(f"ğŸ“º Processing video summary request: {request.url}")
    
    # Generate cache key
    cache_key_data = f"{request.url}|{request.custom_prompt or 'default'}"
    cache_key = hashlib.md5(cache_key_data.encode()).hexdigest()
    cache_file = CACHE_DIR / f"summary_{cache_key}.json"
    
    view_base_url = "https://be.0xfanslab.com/youtube/channel/summary"
    
    # Check cache
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                print(f"âœ… Summary cache HIT for video: {request.url[:50]}...")
                result = cached_data["result"]
                result["view_url"] = f"{view_base_url}?id={cache_key}"
                return result
        except Exception as e:
            print(f"âš ï¸ Cache read error: {e}")
    
    print(f"âŒ Summary cache MISS for video: {request.url[:50]}...")
    
    try:
        # Get video subtitles
        print(f"ğŸ“ Fetching subtitles for: {request.url}")
        video_request = VideoRequest(url=request.url)
        subtitle_result = get_subtitles(video_request)
        
        # Format subtitle text with timestamps
        subtitle_text = " ".join([f"[{ts}] {text}" for ts, text in subtitle_result['transcribed_part'].items()])
        
        # Prepare content for summarization
        video_content = f"å½±ç‰‡: {subtitle_result['title']}\nå…§å®¹: {subtitle_text}"
        
        # Generate AI summary
        print("ğŸ¤– Generating AI summary...")
        summary, chunks = summarize_with_claude(video_content, request.custom_prompt)
        
        # Prepare result
        result = {
            "video_url": request.url,
            "video_id": subtitle_result['video_id'],
            "title": subtitle_result['title'],
            "summary": summary,
            "chunks": chunks,
            "generated_at": datetime.now().isoformat(),
            "raw": video_content,
            "view_url": f"{view_base_url}?id={cache_key}"
        }
        
        # Save to cache
        try:
            cache_data = {
                "request": {
                    "url": request.url,
                    "custom_prompt": request.custom_prompt
                },
                "cached_at": datetime.now().isoformat(),
                "result": result
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved summary to cache: {cache_file.name}")
            
            # Save chunk list separately if chunks exist
            if chunks:
                chunk_list_file = CACHE_DIR / f"chunk_list_{cache_key}.json"
                with open(chunk_list_file, 'w', encoding='utf-8') as f:
                    json.dump({"chunks": chunks}, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ Saved chunk list to cache: {chunk_list_file.name}")
                
        except Exception as e:
            print(f"âš ï¸ Failed to save summary cache: {e}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error in video summarization: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Video summarization failed: {str(e)}"
        )

class ChannelSummaryRequest(BaseModel):
    url: str
    max_videos: int = 5
    custom_prompt: Optional[str] = None

@app.post("/youtube/channel/summary")
def summarize_youtube_channel(request: ChannelSummaryRequest):
    """Summarize recent videos from a YouTube channel"""
    print(f"ğŸ“º Processing channel summary request: {request.url}")
    
    max_videos = min(request.max_videos, 10)
    
    cache_key_data = f"{request.url}|{max_videos}|{request.custom_prompt or 'default'}"
    cache_key = hashlib.md5(cache_key_data.encode()).hexdigest()
    cache_file = CACHE_DIR / f"summary_{cache_key}.json"
    
    view_base_url = "https://be.0xfanslab.com/youtube/channel/summary"
    
    # Get current latest videos to check if cache is still valid
    try:
        current_videos = get_channel_videos(request.url, max_videos)
        latest_video_urls = [video['url'] for video in current_videos]
        print(f"ğŸ“¡ Current latest video(s): {len(latest_video_urls)} found")
    except Exception as e:
        print(f"âš ï¸ Failed to fetch current videos: {e}")
        current_videos = None
        latest_video_urls = None
    
    # Check cache with freshness validation
    if cache_file.exists() and latest_video_urls:
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                
                cached_video_urls = cached_data.get("latest_video_urls", [])
                
                if cached_video_urls == latest_video_urls:
                    print(f"âœ… Summary cache HIT for channel: {request.url[:50]}...")
                    print(f"âœ… Cache is fresh (latest video unchanged)")
                    result = cached_data["result"]
                    result["view_url"] = f"{view_base_url}?id={cache_key}"
                    return result
                else:
                    print(f"ğŸ”„ Cache exists but STALE (new video detected)")
                    print(f"   Cached: {cached_video_urls}")
                    print(f"   Current: {latest_video_urls}")
        except Exception as e:
            print(f"âš ï¸ Cache read error: {e}")
    elif cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                print(f"âœ… Summary cache HIT (freshness check skipped)")
                result = cached_data["result"]
                result["view_url"] = f"{view_base_url}?id={cache_key}"
                return result
        except Exception as e:
            print(f"âš ï¸ Cache read error: {e}")
            
    print(f"âŒ Summary cache MISS for channel: {request.url[:50]}...")
    
    try:
        if current_videos is None:
            videos = get_channel_videos(request.url, max_videos)
        else:
            videos = current_videos
        
        if not videos:
            raise HTTPException(status_code=404, detail="No videos found in channel")
        
        video_contents = []
        processed_videos = []
        
        for video in videos:
            try:
                print(f"ğŸ“ Processing: {video['title']}")
                
                video_request = VideoRequest(url=video['url'])
                subtitle_result = get_subtitles(video_request)
                
                subtitle_text = " ".join([f"[{ts}] {text}" for ts, text in subtitle_result['transcribed_part'].items()])
                
                video_contents.append({
                    'title': video['title'],
                    'url': video['url'],
                    'content': subtitle_text  # No limit
                })
                
                processed_videos.append({
                    'title': video['title'],
                    'url': video['url'],
                    'has_subtitles': True
                })
                
            except Exception as e:
                print(f"âš ï¸  Failed to process {video['title']}: {e}")
                processed_videos.append({
                    'title': video['title'],
                    'url': video['url'],
                    'has_subtitles': False,
                    'error': str(e)
                })
        
        if not video_contents:
            raise HTTPException(
                status_code=404,
                detail="No subtitles found in any of the videos"
            )
        
        combined_content = "\n\n".join([
            f"å½±ç‰‡: {v['title']}\nå…§å®¹: {v['content']}"
            for v in video_contents
        ])
        
        print("ğŸ¤– Generating AI summary...")
        summary, chunks = summarize_with_claude(combined_content, request.custom_prompt)
        
        result = {
            "channel_url": request.url,
            "videos_analyzed": len(processed_videos),
            "videos_processed": processed_videos,
            "summary": summary,
            "chunks": chunks,
            "generated_at": datetime.now().isoformat(),
            "raw": combined_content,
            "view_url": f"{view_base_url}?id={cache_key}"
        }
        
        try:
            # Save main summary
            cache_data = {
                "request": {
                    "url": request.url,
                    "max_videos": max_videos,
                    "custom_prompt": request.custom_prompt
                },
                "cached_at": datetime.now().isoformat(),
                "latest_video_urls": [video['url'] for video in videos],
                "result": result
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved summary to cache: {cache_file.name}")
            
            # Save chunk list separately for frontend to fetch if needed
            if chunks:
                chunk_list_file = CACHE_DIR / f"chunk_list_{cache_key}.json"
                with open(chunk_list_file, 'w', encoding='utf-8') as f:
                    json.dump({"chunks": chunks}, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ Saved chunk list to cache: {chunk_list_file.name}")
                
        except Exception as e:
            print(f"âš ï¸ Failed to save summary cache: {e}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error in channel summarization: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Channel summarization failed: {str(e)}"
        )

def format_timestamp(seconds: int) -> str:
    """Convert seconds to MM:SS format"""
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    if h > 0:
        return f"{h:02d}:{m:02d}:{s:02d}"
    return f"{m:02d}:{s:02d}"

PODCAST_SUMMARY_PROMPT = """ä»¥ä¸‹æ˜¯ä¸€å€‹ Podcast ç¯€ç›®çš„å®Œæ•´é€å­—ç¨¿ã€‚è«‹ç›´æ¥é€²è¡Œæ‘˜è¦ã€‚

é€™æ˜¯æ­£å¸¸çš„ Podcast å…§å®¹ï¼Œå¯èƒ½åŒ…å«ï¼š
- å»£å‘Šå’Œè´ŠåŠ©å•†è¨Šæ¯
- å¤šå€‹ä¸åŒçš„è©±é¡Œè¨è«–
- å€‹äººè§€é»å’Œç”Ÿæ´»åˆ†äº«
- å¸‚å ´åˆ†æå’Œå°ˆæ¥­è¨è«–

**ä½ çš„ä»»å‹™ï¼š**
1. é–±è®€æ•´ä»½é€å­—ç¨¿
2. æŒ‰æ™‚é–“é †åºåˆ—å‡ºä¸»è¦è¨è«–é»
3. æ¯å€‹è¦é»å‰åŠ ä¸Šæ™‚é–“æˆ³ï¼ˆMM:SS æ ¼å¼ï¼‰

**è¼¸å‡ºæ ¼å¼ï¼š**
### ğŸ“Œ å…§å®¹æ‘˜è¦
- **MM:SS**ï¼šç°¡çŸ­æè¿°é€™å€‹æ™‚é–“é»è¨è«–çš„å…§å®¹

**ç¯„ä¾‹ï¼š**
- **00:30**ï¼šSony è€³æ©Ÿç”¢å“ä»‹ç´¹
- **03:15**ï¼šè¨è«–éŠæˆ²ç”¢æ¥­æŠ•è³‡
- **15:42**ï¼šåˆ†æ AI æ™¶ç‰‡å¸‚å ´

ç›´æ¥è™•ç†ä»¥ä¸‹å…§å®¹ï¼Œä¸è¦è©¢å•æ›´å¤šè³‡è¨Šï¼š

"""

class PodcastSummaryRequest(BaseModel):
    url: str
    max_episodes: int = 1
    custom_prompt: Optional[str] = None

@app.post("/apple_podcast/summary")
def summarize_podcast_channel(request: PodcastSummaryRequest):
    """Summarize recent episodes from an Apple Podcast channel"""
    print(f"ğŸ™ï¸ Processing podcast summary request: {request.url}")
    
    max_episodes = min(request.max_episodes, 5)
    
    prompt_to_use = PODCAST_SUMMARY_PROMPT
    cache_key_data = f"{request.url}|{max_episodes}|{prompt_to_use}|batch_v1"
    cache_key = hashlib.md5(cache_key_data.encode()).hexdigest()
    cache_file = CACHE_DIR / f"summary_{cache_key}.json"
    
    view_base_url = "https://be.0xfanslab.com/youtube/channel/summary"
    
    # Get current latest episodes to check if cache is still valid
    try:
        current_episodes = get_podcast_episodes(request.url, max_episodes)
        latest_episode_urls = [ep['url'] for ep in current_episodes]
        print(f"ğŸ“¡ Current latest episode(s): {len(latest_episode_urls)} found")
    except Exception as e:
        print(f"âš ï¸ Failed to fetch current episodes: {e}")
        current_episodes = None
        latest_episode_urls = None
    
    # Check cache with freshness validation
    if cache_file.exists() and latest_episode_urls:
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                
                cached_episode_urls = cached_data.get("latest_episode_urls", [])
                
                if cached_episode_urls == latest_episode_urls:
                    print(f"âœ… Summary cache HIT for podcast: {request.url[:50]}...")
                    print(f"âœ… Cache is fresh (latest episode unchanged)")
                    result = cached_data["result"]
                    result["view_url"] = f"{view_base_url}?id={cache_key}"
                    return result
                else:
                    print(f"ğŸ”„ Cache exists but STALE (new episode detected)")
                    print(f"   Cached: {cached_episode_urls}")
                    print(f"   Current: {latest_episode_urls}")
        except Exception as e:
            print(f"âš ï¸ Cache read error: {e}")
    elif cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                print(f"âœ… Summary cache HIT (freshness check skipped)")
                result = cached_data["result"]
                result["view_url"] = f"{view_base_url}?id={cache_key}"
                return result
        except Exception as e:
            print(f"âš ï¸ Cache read error: {e}")
            
    print(f"âŒ Summary cache MISS for podcast: {request.url[:50]}...")
    
    try:
        if current_episodes is None:
            episodes = get_podcast_episodes(request.url, max_episodes)
        else:
            episodes = current_episodes
        
        episode_contents = []
        processed_episodes = []
        
        for episode in episodes:
            try:
                print(f"ğŸ“ Processing: {episode['title']}")
                
                episode_request = VideoRequest(url=episode['url'])
                subtitle_result = get_apple_podcast_subtitles(episode_request)
                
                subtitle_text = " ".join([
                    f"[{format_timestamp(int(ts))}] {text}" 
                    for ts, text in subtitle_result['transcribed_part'].items()
                ])
                
                episode_contents.append({
                    'title': episode['title'],
                    'url': episode['url'],
                    'content': subtitle_text
                })
                
                processed_episodes.append({
                    'title': episode['title'],
                    'url': episode['url'],
                    'has_subtitles': True
                })
                
            except Exception as e:
                print(f"âš ï¸ Failed to process {episode['title']}: {e}")
                processed_episodes.append({
                    'title': episode['title'],
                    'url': episode['url'],
                    'has_subtitles': False,
                    'error': str(e)
                })
        
        if not episode_contents:
            raise HTTPException(status_code=404, detail="No transcripts found")
            
        combined_content = "\n\n".join([
            f"é›†æ•¸: {e['title']}\nå…§å®¹: {e['content']}"
            for e in episode_contents
        ])
        
        print("ğŸ¤– Generating AI summary...")
        summary, chunks = summarize_with_claude(combined_content, prompt_to_use)
        
        result = {
            "channel_url": request.url,
            "videos_analyzed": len(processed_episodes),
            "videos_processed": processed_episodes,
            "summary": summary,
            "chunks": chunks,
            "generated_at": datetime.now().isoformat(),
            "raw": combined_content,
            "view_url": f"{view_base_url}?id={cache_key}"
        }
        
        try:
            cache_data = {
                "request": {
                    "url": request.url,
                    "max_episodes": max_episodes,
                    "custom_prompt": request.custom_prompt
                },
                "cached_at": datetime.now().isoformat(),
                "latest_episode_urls": [ep['url'] for ep in episodes],
                "result": result
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved podcast summary to cache: {cache_file.name}")
            
            # Save chunk list separately for frontend to fetch if needed
            if chunks:
                chunk_list_file = CACHE_DIR / f"chunk_list_{cache_key}.json"
                with open(chunk_list_file, 'w', encoding='utf-8') as f:
                    json.dump({"chunks": chunks}, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ Saved chunk list to cache: {chunk_list_file.name}")
                
        except Exception as e:
            print(f"âš ï¸ Failed to save cache: {e}")
        
        return result

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error in podcast summarization: {e}")
        raise HTTPException(status_code=500, detail=f"Podcast summarization failed: {str(e)}")

@app.get("/cache/stats")
def get_cache_stats():
    """Get cache statistics"""
    cache_files = list(CACHE_DIR.glob("*.json"))
    total_size = sum(f.stat().st_size for f in cache_files)
    
    return {
        "total_files": len(cache_files),
        "total_size_bytes": total_size,
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "cache_directory": str(CACHE_DIR)
    }

@app.delete("/cache/clear")
def clear_cache():
    """Clear all cache files"""
    cache_files = list(CACHE_DIR.glob("*.json"))
    count = 0
    
    for f in cache_files:
        try:
            f.unlink()
            count += 1
        except Exception as e:
            print(f"Failed to delete {f}: {e}")
    
    return {
        "deleted_files": count,
        "message": f"Cleared {count} cache files"
    }

@app.get("/api/summary/{cache_key}")
def get_summary_cache(cache_key: str):
    """Retrieve cached summary by cache key"""
    cache_file = CACHE_DIR / f"summary_{cache_key}.json"
    
    if not cache_file.exists():
        raise HTTPException(status_code=404, detail="Cache not found")
    
    try:
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
            return cache_data.get("result", {})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read cache: {str(e)}")

@app.get("/api/chunks/{cache_key}")
def get_chunk_list(cache_key: str):
    """Retrieve chunk list by cache key"""
    chunk_list_file = CACHE_DIR / f"chunk_list_{cache_key}.json"
    
    if not chunk_list_file.exists():
        # Fallback: check if chunks are embedded in the main summary cache
        cache_file = CACHE_DIR / f"summary_{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    result = cache_data.get("result", {})
                    if "chunks" in result and result["chunks"]:
                        return {"chunks": result["chunks"]}
            except:
                pass
        
        raise HTTPException(status_code=404, detail="Chunks not found")
    
    try:
        with open(chunk_list_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read chunk list: {str(e)}")


# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/youtube/channel/summary")
def serve_summary_viewer():
    """Serve the summary viewer page"""
    return FileResponse("static/summary_viewer.html")

@app.get("/twstock/{stock_id}")
def get_taiwan_stock(stock_id: str):
    """Query Taiwan Stock Exchange API for stock information"""
    print(f"ğŸ“ˆ Querying Taiwan stock: {stock_id}")
    
    try:
        # TWSE API endpoint for real-time stock info
        # This endpoint provides real-time stock information from Taiwan Stock Exchange
        url = f"https://mis.twse.com.tw/stock/api/getStockInfo.jsp?ex_ch=tse_{stock_id}.tw"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        # Check if we got valid data
        # TSE returns incomplete data (only tv, s, c, z fields) when stock is not found in TSE
        if 'msgArray' not in data or len(data['msgArray']) == 0 or 'n' not in data['msgArray'][0]:
            # Try OTC (over-the-counter) market
            url_otc = f"https://mis.twse.com.tw/stock/api/getStockInfo.jsp?ex_ch=otc_{stock_id}.tw"
            response_otc = requests.get(url_otc, headers=headers, timeout=10)
            response_otc.raise_for_status()
            data_otc = response_otc.json()
            
            if 'msgArray' not in data_otc or len(data_otc['msgArray']) == 0 or 'n' not in data_otc['msgArray'][0]:
                raise HTTPException(
                    status_code=404,
                    detail=f"Stock {stock_id} not found in TWSE or OTC markets"
                )
            data = data_otc
        
        stock_info = data['msgArray'][0]
        
        # Format the response with useful information
        result = {
            "stock_id": stock_id,
            "name": stock_info.get('n', 'N/A'),  # Stock name
            "full_name": stock_info.get('nf', 'N/A'),  # Full name
            "current_price": stock_info.get('z', 'N/A'),  # Current price
            "opening_price": stock_info.get('o', 'N/A'),  # Opening price
            "highest_price": stock_info.get('h', 'N/A'),  # Highest price
            "lowest_price": stock_info.get('l', 'N/A'),  # Lowest price
            "yesterday_price": stock_info.get('y', 'N/A'),  # Yesterday's closing price
            "change": stock_info.get('z', 'N/A'),  # Price change
            "volume": stock_info.get('v', 'N/A'),  # Trading volume
            "timestamp": stock_info.get('t', 'N/A'),  # Timestamp
            "exchange": stock_info.get('ex', 'N/A'),  # Exchange (tse/otc)
            "raw_data": stock_info  # Include raw data for reference
        }
        
        # Calculate price change and percentage if possible
        try:
            if result['current_price'] != 'N/A' and result['yesterday_price'] != 'N/A':
                current = float(result['current_price'])
                yesterday = float(result['yesterday_price'])
                change = current - yesterday
                change_percent = (change / yesterday) * 100
                result['price_change'] = round(change, 2)
                result['price_change_percent'] = round(change_percent, 2)
        except (ValueError, ZeroDivisionError):
            pass
        
        print(f"âœ… Successfully retrieved stock info for {stock_id}: {result.get('name', 'N/A')}")
        return result
        
    except requests.RequestException as e:
        print(f"âŒ Error fetching stock data: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch stock data from TWSE API: {str(e)}"
        )
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
